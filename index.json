[{"content":"\u003ch1 id=\"projects--home\"\u003ePROJECTS @ HOME\u003c/h1\u003e\n\u003ch2 id=\"the-time-i-accidentally-built-my-own-cloud-platform\"\u003eThe Time I Accidentally Built My Own \u0026ldquo;Cloud\u0026rdquo; Platform\u003c/h2\u003e\n\u003cp\u003eDuring the pandemic I leveraged the fact that I\u0026rsquo;m having deployed hardware on three different, geo-distributed \u003cem\u003eregions\u003c/em\u003e so I semi-consciounsly built my own highly-available, on-premise \u0026ldquo;Cloud\u0026rdquo; platform.\u003c/p\u003e\n\u003cp\u003eI used existing software components I already have knowledge in and additionally wrote some software myself. Basic building blocks, such as blob storage, databases, service discovery and Kubernetes nodes are highly-available through (almost) immutable VMs based on a terraformed libvirt API.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/img/soeren-platform.drawio.png\"\u003e\u003cimg src=\"/img/soeren-platform.drawio.png\" alt=\"imagen\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"self-written-software\"\u003eSELF-WRITTEN SOFTWARE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/ssh-key-signer\"\u003eManage SSH (host) certificates using Hashicorp Vault\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/vault-pki-cli\"\u003eManage client certificates using Hashicorp Vault\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/scripts/blob/main/vault/vault_approle_cli.py\"\u003eAutomatically manage Vault AppRoles\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/occult\"\u003eAutomatically unlock stuff using Vault\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/acmevault\"\u003eRequest x509 certs via LetsEncrypt and distribute via Vault to private hosts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/dyndns\"\u003eDetect IP changes and update predefined DNS records through signed messages via MQTT\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"apps-layer\"\u003eAPPS LAYER\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHighly available monitoring running Prometheus stack\u003c/li\u003e\n\u003cli\u003eFederated Prometheus cluster on top of Cortex\u003c/li\u003e\n\u003cli\u003eLogging based on Loki\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"platform-layer\"\u003ePLATFORM LAYER\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRunning a Hashicorp Vault cluster configured using \u003ca href=\"https://github.com/soerenschneider/tf-vault\"\u003eterraform\u003c/a\u003e. Used for\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.vaultproject.io/docs/secrets/identity\"\u003emachine identities\u003c/a\u003e, the foundation of every authentication and authorization\u003c/li\u003e\n\u003cli\u003einternal PKIs, using x509 for client and host certificates\u003c/li\u003e\n\u003cli\u003eSSH certificates, all SSH authentication is based on certificates instead of authorized keys\u003c/li\u003e\n\u003cli\u003estatic secrets, such as API tokens\u003c/li\u003e\n\u003cli\u003eAWS secrets engine, dynamically create short-lived access tokens\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHighly available Minio Cluster running in distributed mode, installed using Ansible, configured via Terraform\u003c/li\u003e\n\u003cli\u003eRunning highly available MariaDB Galera cluster, configured using Ansible\u003c/li\u003e\n\u003cli\u003eSingle Sign On provided by Keycloak using OIDC, configured by terraform. Not highly available, yet\u003c/li\u003e\n\u003cli\u003eRunning highly available Kubernetes cluster installed using Kubespray, configured using GitOps\u003c/li\u003e\n\u003cli\u003eService discovery based on Consul, experiments ongoing in regards of Ansible dynamic inventory\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"infrastructure-layer\"\u003eINFRASTRUCTURE LAYER\u003c/h3\u003e\n\u003ch4 id=\"openbsd-router\"\u003eOpenBSD Router\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAll routers are built on top of \u003ca href=\"https://www.pcengines.ch/apu2.htm\"\u003eAPU2\u003c/a\u003e boards, running \u003ca href=\"https://gitlab.com/bconway/resflash\"\u003eresflash\u003c/a\u003e to provide (almost) immutable infrastructure\u003c/li\u003e\n\u003cli\u003eRouters configured using Ansible\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"linux-virtual-server-hosts\"\u003eLinux Virtual Server Hosts\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eThe host systems expose a minimal attack surface and only run libvirtd and a sshd server\u003c/li\u003e\n\u003cli\u003eAutomatic patch management in place\u003c/li\u003e\n\u003cli\u003eVirtual machines are solely configured using \u003ca href=\"https://github.com/soerenschneider/tf-libvirt\"\u003eterraform\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eEach machine has an associated \u003ca href=\"https://www.vaultproject.io/docs/secrets/identity\"\u003eidentity\u003c/a\u003e enabling zero trust networking\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"notification-on-late-trams\"\u003eNotification on Late Trams\u003c/h2\u003e\n\u003cp\u003eWhile I was living in Cologne I realized the local public transportation company enjoyed being.. on the spontaneous side of life, so I created an overengineered solution to notify me on late trams. I ended up with 4 microservices that communicate via loosely coupled streams based on redis.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/kfailb-scraper\"\u003escraping tram data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/kfailb-parser\"\u003eparsing scraped data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/kfailb-bot\"\u003ededuplication and notifying on late tram via telegram bot\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/k-fail-banal\"\u003estoring data long term\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"home-automation\"\u003eHome Automation\u003c/h2\u003e\n\u003cp\u003eInstead of relying on a commercial vendor, I spent some time on my DIY home automation based on Home Assistant and my fleet of Raspberry Zeros, Arduinos \u0026amp; ESP2866s. I\u0026rsquo;ve re-written tools based on the excellent gobot framework to scrape and distribute all the sensor data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/gobot-pir\"\u003ereading motion data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/gobot-bme280/\"\u003ereading temperature data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/soerenschneider/gobot-lux\"\u003ereading brightness data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/pir-motion-raspberry\"\u003e(deprecated) reading motion sensor data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/mqtt-tempsensor\"\u003e(deprecated) reading temperature sensor data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gitlab.com/soerenschneider/serial-sensor-reader\"\u003e(deprecated) reading arbitrary analog sensor data over serial line\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"projects--work\"\u003ePROJECTS @ WORK\u003c/h1\u003e\n\u003ch3 id=\"chaos-engineering\"\u003eChaos Engineering\u003c/h3\u003e\n\u003cp\u003eCo-founder of the Chaos Engineering guild that serves as force for the adoption of Chaos Engineering principles at our employer, Rewe Digital. We evaluate tools in the context of Chaos Engineering, provide introduction lessons \u0026amp; workshops, do consultation work and moderate our teams' Gamedays. After hosting several Gamedays for other teams, we also started offering Firedrills, based on common scenarios across our tech-stack (how much is your backup scenario worth if you don\u0026rsquo;t know if it works?). Further, we started finding the biggest risks of our own components by performing SFMEAs and plan to offer SFMEA consulting for other teams as well.\u003c/p\u003e\n\u003ch3 id=\"github-enterpise-migration\"\u003eGitHub Enterpise Migration\u003c/h3\u003e\n\u003cp\u003eWe worked on the conception \u0026amp; realization of a modern, scalable, self-hosted GitHub Actions Runner infrastructure. Building and operating a service that regularly rotates individual runner tokens for all business units. Providing customized tooling (Terraform \u0026amp; POSIX) to support our engineers to migrate more than 3500 repositories from Bitbucket to GitHub with minimal friction.\u003c/p\u003e\n\u003ch3 id=\"google-iap-kubernetes-controller\"\u003eGoogle IAP Kubernetes Controller\u003c/h3\u003e\n\u003cp\u003eTo allow an uncomplicated exposal of Kubernetes services via a secured Google Identity Aware Proxy, we wrote a Kubernetes Controller that solely works on appropriate labels on the K8s service resources. To be 100% sure that the service is not being exposed publicly (in case Google IAP has trouble), an \u003ca href=\"https://www.envoyproxy.io/\"\u003eenvoy proxy\u003c/a\u003e is injected as a sidecar that performs mandatory JWT verification.\u003c/p\u003e\n\u003ch3 id=\"synchronizing-secrets-from-hashicorp-vault\"\u003eSynchronizing secrets from Hashicorp Vault\u003c/h3\u003e\n\u003cp\u003eWe wrote a critical component that reads secrets from Hashicorp Vault and synchronizes them unidirectionally to interchangeable backend. It is possible to consume (re-read) a certain secret on each interval or reading the secret once and extending its lease variably before its expiry. The provided backends currently support writing the secret as a Kubernetes secret and writing as a file on Cloud Storage bucket.\u003c/p\u003e\n\u003ch3 id=\"graceful-pod-killer\"\u003eGraceful Pod Killer\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThere are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eI wrote a tool that is able to safely restart targeted Deployments. Targets can be auto-discovered by (multiple) labels, a certain secret they have mounted and/or their start date. After killing a pod, it waits until the replica is healthy again to continue with the next pod in line. This tool comes in handy for controlled chaos engineering operations or when you need to restart pods to force the consumption of an updated secret.\u003c/p\u003e\n\u003ch3 id=\"automatically-verifying-cloud-sql-backups\"\u003eAutomatically verifying Cloud SQL backups\u003c/h3\u003e\n\u003cp\u003eOn more than one occasion we were facing incidents with Cloud SQL instances that got into a bad state and whose last \u003cem\u003en\u003c/em\u003e out of \u003cem\u003em\u003c/em\u003e backups were unusable. We decided to build a product using multiple Cloud Functions that automatically restores new Cloud SQL backups, verifies them, cleans up everything and notifies the owning team in case of problems.\u003c/p\u003e\n\u003ch3 id=\"get-notified-on-outdated-docker-images-in-kubernetes\"\u003eGet notified on outdated Docker images in Kubernetes\u003c/h3\u003e\n\u003cp\u003eDuring several \u0026lsquo;Learning Friday\u0026rsquo; sessions I wrote a POC that checks whether running pods use outdated docker images. To inspect a cluster, an agent is deployed that gathers the needed information via internal Kubernetes API calls and reports back via Pub Sub to the master. In the master, the docker image string is disassembled. After that, a call against the appropriate image registry\u0026rsquo;s API is performed, checking out the available versions and writes the information to a SQL database. Versions were compared and on a version skew, teams were notified via prometheus.\u003c/p\u003e\n\u003ch3 id=\"cloud-functions-metrics-exporter\"\u003eCloud Functions metrics exporter\u003c/h3\u003e\n\u003cp\u003eWhen using Cloud Functions we wanted to be able to use fine-grained monitoring for our functions. Due to the nature of Cloud Functions still being early beta, it wasn\u0026rsquo;t possible to peer them to our GKE network. Therefore we weren\u0026rsquo;t able to use Prometheus Pushgateway (as it is exposed only inside the clusters) or scrape them like regular targets (due to their low runtime, networking, \u0026hellip;).\u003c/p\u003e\n\u003cp\u003eAs the functions were all triggered using Pub Sub, we decided to write a Prometheus exporter that receives metrics in raw Prometheus text format dispatched via Pub Sub once the function finished.\u003c/p\u003e\n\u003ch3 id=\"aws-cloud-architecture--migration\"\u003eAWS Cloud Architecture \u0026amp; Migration\u003c/h3\u003e\n\u003cp\u003eThe most challenging task undoubtedly was planning and migrating to the cloud architecture at AWS. Due to multiple constellations, the end date of the migration was already known before real planning started. After \u003cem\u003ea lot\u003c/em\u003e of small, nightly iterations the transition was done, but not the work. The initial move involved almost only manual work, after the move to AWS I started to replicate the environment using IAC.\u003c/p\u003e\n\u003cp\u003eWith an ever increasing size of both team members and services, conflicts of the monolithic architecture became apparent. The first container orchestration used, was ECS. Due to its nature, being as dull as solid, it is the ideal solution for understaffed departments. Later on, there was the big shift towards a kops-created Kubernetes, unfortunately operated only by me.\u003c/p\u003e\n\u003ch3 id=\"business-partner-data-curation\"\u003eBusiness Partner Data Curation\u003c/h3\u003e\n\u003cp\u003eConcept and realization of an easily configurable, high-performance concurrent service that improves business partner data. The data was read from a queue, ran through several \u003cem\u003ecuration elements\u003c/em\u003e and was written to a new MongoDB collection. The list of elements and their order was dymanic and interchangable. A single data element could take various paths down the curator based on its attributes, e.g. a certain country, a certain \u0026ldquo;legal form\u0026rdquo;. A curation element could alter (e.g. transliteration) and also flag (e.g. \u0026ldquo;legal form is invalid for given country\u0026rdquo;) data. Several permutations of configuration could be performed, to find the best settings for input data.\u003c/p\u003e\n\u003ch3 id=\"providing-scalable-enterprise-ready-semantic-mediawiki-stacks\"\u003eProviding scalable, enterprise-ready Semantic MediaWiki stacks\u003c/h3\u003e\n\u003cp\u003eUsing both CloudFormation and Ansible I was able to roll-out a scalable \u003ca href=\"https://www.semantic-mediawiki.org/wiki/Semantic_MediaWiki\"\u003eSemantic Media Wiki\u003c/a\u003e providing Single Sign On capabilities with few effort. The resulting VMs were part of Auto Scaling Groups and were backed by Docker images. SPARQL queries were answered by an Apache Fuseki.\u003c/p\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/projects/","title":"PROJECTS"},{"content":"\u003ch1 id=\"abstract\"\u003eABSTRACT\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e10+ years\u003c/strong\u003e of working experience in the industry\u003c/li\u003e\n\u003cli\u003eStarted out as \u003cstrong\u003eSoftware-Engineer\u003c/strong\u003e and later \u003cstrong\u003eSoftware-Architect\u003c/strong\u003e, working more closely to \u003cstrong\u003eCloud Infrastructures\u003c/strong\u003e for the last 5+ years\u003c/li\u003e\n\u003cli\u003eGenerally interested in Software Resilience, Distributed Systems and IT Security\u003c/li\u003e\n\u003cli\u003eKnowledge (different levels) in AWS, Azure \u0026amp; Google Cloud Platform\u003c/li\u003e\n\u003cli\u003eWriting software preferably in Golang and Python\u003c/li\u003e\n\u003cli\u003eCurrently getting deeper into Azure and trying to tackle the steep learning curve of Rust\u0026rsquo;s lifetimes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr/\u003e\n\u003cbr/\u003e\n\u003ch1 id=\"senior-cloud-engineer--sap-fioneer-remotehttpswwwsapfioneercom-122021----now\"\u003e\u003ca href=\"https://www.sapfioneer.com/\"\u003eSENIOR CLOUD ENGINEER @ SAP FIONEER (Remote)\u003c/a\u003e, 12/2021 \u0026ndash; NOW\u003c/h1\u003e\n\u003ch3 id=\"core-objectives\"\u003eCORE OBJECTIVES\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHelp shaping and implementing the MVP of SAP Fioneer\u0026rsquo;s \u003cem\u003eCommercial Lending\u003c/em\u003e product\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"tasks\"\u003eTASKS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eInvolved in building the company\u0026rsquo;s upcoming Azure architecture\u003c/li\u003e\n\u003cli\u003eRealized secrets management based on Azure Key Vault, GitHub Actions and Terraform\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"technologies\"\u003eTECHNOLOGIES\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGolang, Python\u003c/li\u003e\n\u003cli\u003eAzure\n\u003cul\u003e\n\u003cli\u003eKubernetes w. Istio / AKS\u003c/li\u003e\n\u003cli\u003eAzure SQL (Postgres)\u003c/li\u003e\n\u003cli\u003eAAD\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGraphQL, REST, OData\u003c/li\u003e\n\u003cli\u003eTerraform\u003c/li\u003e\n\u003cli\u003eGitHub Actions\n\u003cul\u003e\n\u003cli\u003eSnyk Container\u003c/li\u003e\n\u003cli\u003eCodacy\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSAP BTP\u003c/li\u003e\n\u003cli\u003eKotlin w. Spring Boot\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"cloud-engineer--rd-cologne--hybridhttpswwwrewe-digitalcom-1118\"\u003e\u003ca href=\"https://www.rewe-digital.com/\"\u003eCLOUD ENGINEER @ RD (Cologne / Hybrid)\u003c/a\u003e, 11/18\u003c/h1\u003e\n\u003cp\u003eAfter 4 years, I moved on to work as a Cloud Engineer at Rewe Digital, helping the company digitize the retail food sector in Germany. I was eager to both expand my Cloud expertise on the Google Cloud Platform and also grow with a dedicated expert team of Cloud Engineers.\u003c/p\u003e\n\u003ch3 id=\"core-objectives-1\"\u003eCORE OBJECTIVES\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eBuilding, continously improving and maintaining the company\u0026rsquo;s large fulfillment platform\u003c/li\u003e\n\u003cli\u003eEnabling and offering consultation to feature teams to efficiently run their payloads on our platform\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"tasks-1\"\u003eTASKS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEstablishing Chaos Engineering and SRE-methologies within the company\n\u003cul\u003e\n\u003cli\u003ePerforming Chaos Engineering gamedays, both internally and with fellow teams\u003c/li\u003e\n\u003cli\u003eRisk management using SFMEA\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eProviding building blocks that solve common platform problems\n\u003cul\u003e\n\u003cli\u003eBoth dogfooding and relying on existing, battle-proven open source software\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOffering trainings for feature teams\n\u003cul\u003e\n\u003cli\u003eGoogle Cloud Platform 101\u003c/li\u003e\n\u003cli\u003eKubernetes 101\u003c/li\u003e\n\u003cli\u003eChaos Engineering\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePerformed smoothless migration \u0026amp; integration from Bitbucket to Github Enterprise Integration\n\u003cul\u003e\n\u003cli\u003eProviding a modern \u0026amp; scalable locally hosted GitHub Actions Runners setup\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"technologies-1\"\u003eTECHNOLOGIES\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eGolang, Python, Shell scripts\u003c/li\u003e\n\u003cli\u003eGoogle Cloud\n\u003cul\u003e\n\u003cli\u003eKubernetes (GKE)\u003c/li\u003e\n\u003cli\u003eGCE\u003c/li\u003e\n\u003cli\u003eIAP\u003c/li\u003e\n\u003cli\u003eCloud SQL\u003c/li\u003e\n\u003cli\u003eCloud Functions\u003c/li\u003e\n\u003cli\u003eDatastore\u003c/li\u003e\n\u003cli\u003ePub Sub\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVault\u003c/li\u003e\n\u003cli\u003eTerraform, Packer, Ansible\u003c/li\u003e\n\u003cli\u003ePrometheus-stack, Grafana\u003c/li\u003e\n\u003cli\u003eGitHub Enterprise, GitHub Actions, Bitbucket, Jenkins\u003c/li\u003e\n\u003cli\u003eLitmus Chaos, ChaosMesh, Chaos Monkey for Spring Boot\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"cloud-operations-manager--cdq-remotehttpswwwcdqchdata-sharingdata-sharing-community-0117--1118\"\u003e\u003ca href=\"https://www.cdq.ch/data-sharing/data-sharing-community\"\u003eCLOUD OPERATIONS MANAGER @ CDQ (Remote)\u003c/a\u003e, 01/17\u0026ndash;11/18\u003c/h1\u003e\n\u003cp\u003eWhen Startups don\u0026rsquo;t offer monetary compensation, they\u0026rsquo;ll compensate with funny titles on business cards and a steep learning curve. From January 2017 on, I was promoted internally to be the sole responsible person for migrating our cloud presence to AWS and establishing a professional, modern cloud architecture built on AWS.\u003c/p\u003e\n\u003ch3 id=\"core-objectives-2\"\u003eCORE OBJECTIVES\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eProfessionalizing the company\u0026rsquo;s cloud infrastructure\u003c/li\u003e\n\u003cli\u003eSupporting and thus facilitating the company\u0026rsquo;s rapid growth by providing highly scalable infrastructure\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"responsibilities--tasks\"\u003eRESPONSIBILITIES / TASKS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSole responsible person for providing and operating a scalable, robust platform\n\u003cul\u003e\n\u003cli\u003ePlanned and realized AWS cloud architecture\u003c/li\u003e\n\u003cli\u003ePerformed migration from Swisscom Enterprise Cloud Solution to AWS\u003c/li\u003e\n\u003cli\u003eProviding dynamic testing environments using Terraform\u003c/li\u003e\n\u003cli\u003eAnalyzing performance-, security- and network-related issues\u003c/li\u003e\n\u003cli\u003eMonitoring infrastructure- and service health\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContainerized all workloads\u003c/li\u003e\n\u003cli\u003eDriving cloud contextual topics at the Architecture Board\n\u003cul\u003e\n\u003cli\u003eGetting rid of state\u003c/li\u003e\n\u003cli\u003eDealing with network failures\u003c/li\u003e\n\u003cli\u003eSelf-healing software\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTraining for team members\n\u003cul\u003e\n\u003cli\u003eKubernetes basics\u003c/li\u003e\n\u003cli\u003eBuilding Docker images correctly\u003c/li\u003e\n\u003cli\u003eHow to define Prometheus metrics \u0026amp; alerts\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInfrastructure documentation and compliance\u003c/li\u003e\n\u003cli\u003eRealization, verification and instrumentation of backups\u003c/li\u003e\n\u003cli\u003eBuilding and providing developer selfservices\u003c/li\u003e\n\u003cli\u003eCost management\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"technologies-2\"\u003eTECHNOLOGIES\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePython\u003c/li\u003e\n\u003cli\u003eAWS\n\u003cul\u003e\n\u003cli\u003eLambda\u003c/li\u003e\n\u003cli\u003eRDS\u003c/li\u003e\n\u003cli\u003eEC2\u003c/li\u003e\n\u003cli\u003eECS\u003c/li\u003e\n\u003cli\u003eS3\u003c/li\u003e\n\u003cli\u003e(EKS)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes (Kops), Kong API Gateway, Ansible\u003c/li\u003e\n\u003cli\u003eGraylog, Prometheus, Alertmanager, Grafana\u003c/li\u003e\n\u003cli\u003eJenkins, Sonarqube, Nexus\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"software-architect--cdq-remotehttpswwwcdqchdata-sharingdata-sharing-community-0115--0117\"\u003e\u003ca href=\"https://www.cdq.ch/data-sharing/data-sharing-community\"\u003eSOFTWARE ARCHITECT @ CDQ (Remote)\u003c/a\u003e 01/15\u0026ndash;01/17\u003c/h1\u003e\n\u003cp\u003eIn January 2015 I started working for the Switzerland-based startup whose vision is to improve master data quality related to business partners. As I was the first technical employee, I translated and realized all initial business requirements as a working software solution that proved itself over the years. The resulting SaaS platform allowed corporations to share and thus improve their data. Later, more and more related services, such as bank fraud detection and -prevention, were added to the product.\u003c/p\u003e\n\u003ch3 id=\"core-objectives-3\"\u003eCORE OBJECTIVES\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eDelivering a MVP of the data sharing SaaS product\u003c/li\u003e\n\u003cli\u003eEstablishing professional software development structures\u003c/li\u003e\n\u003cli\u003eDefininition and implementation of the architecture\u003c/li\u003e\n\u003cli\u003eGather and translate business requirements into technical solutions\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"responsibilities--tasks-1\"\u003eRESPONSIBILITIES / TASKS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCreating a MVP of the data sharing SaaS product\n\u003cul\u003e\n\u003cli\u003eDefining SaaS components\u003c/li\u003e\n\u003cli\u003eProviding Swisscom Enterprise Cloud Infrastructure\u003c/li\u003e\n\u003cli\u003eAPI Design\u003c/li\u003e\n\u003cli\u003eDomain model design\u003c/li\u003e\n\u003cli\u003eDatabase design\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTranslating business requirements into technical solutions\n\u003cul\u003e\n\u003cli\u003eWriting epics, user stories and decision records\u003c/li\u003e\n\u003cli\u003eDefining architecture and appropriate documentation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDefining software development structures\n\u003cul\u003e\n\u003cli\u003eIntroduced Kanban, alter Scrum\u003c/li\u003e\n\u003cli\u003eSCM branching processes and release management processes\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBuilding CI Pipelines\n\u003cul\u003e\n\u003cli\u003eProviding software testing stack via Docker compose\u003c/li\u003e\n\u003cli\u003eSonarQube ownership\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDocumentation of software architecture and architectural choices\u003c/li\u003e\n\u003cli\u003eRealiziation of smaller side-projects\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"technologies-3\"\u003eTECHNOLOGIES\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eJava, Spring (-boot, -data, -security, -cloud, -batch), Hibernate, RabbitMQ\u003c/li\u003e\n\u003cli\u003eSwisscom Enterprise Cloud, Apache Tomcat\u003c/li\u003e\n\u003cli\u003eJenkins, Bamboo, SonarKube, Docker\u003c/li\u003e\n\u003cli\u003eMySQL, MediaWiki, MongoDB, Redis, ElasticSearch, OpenLDAP, simpleSAML, OpenVPN\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"software-developer--opencms-colognehttpwwwopencmsorgen-0613--1214\"\u003e\u003ca href=\"http://www.opencms.org/en/\"\u003eSOFTWARE DEVELOPER @ OpenCMS (Cologne)\u003c/a\u003e, 06/13\u0026ndash;12/14\u003c/h1\u003e\n\u003cp\u003eBeing driven by the idealistic motive of getting paid working on Open Source software, I got my first full-time job working as a software developer on OpenCMS. My main tasks included\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWorking on OpenCMS\u003c/li\u003e\n\u003cli\u003eVarious works around the Solr-powered full text search module\u003c/li\u003e\n\u003cli\u003eCreation of a full text search web framework using handlebars, GWT and SOLR\u003c/li\u003e\n\u003cli\u003eRealization of smaller projects\u003c/li\u003e\n\u003cli\u003eInitiated first internal tests with Docker\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"technologies-4\"\u003eTECHNOLOGIES\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eJava, Maven, JavaScript, Handlebars, Google Web Toolkit\u003c/li\u003e\n\u003cli\u003eApache Solr\u003c/li\u003e\n\u003cli\u003eDocker\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"research-assistant--fraunhofer-iese-kaiserslauternhttpswwwiesefraunhoferde-08--13\"\u003e\u003ca href=\"https://www.iese.fraunhofer.de/\"\u003eRESEARCH ASSISTANT @ Fraunhofer IESE (Kaiserslautern)\u003c/a\u003e, 08\u0026ndash;13\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGained first experiences working in a professional environment\u003c/li\u003e\n\u003cli\u003eWorked in the context of the European research project \u003ca href=\"http://publica.fraunhofer.de/documents/N-175840.html\"\u003eAdiWa\u003c/a\u003e on a Java-based backend, using REST, Hibernate and MQTT.\u003c/li\u003e\n\u003cli\u003eWorked with \u003ca href=\"https://www.eclipse.org/modeling/emf/\"\u003emodel based code generation\u003c/a\u003e on \u003ca href=\"https://www.iese.fraunhofer.de/en/competencies/architecture/architecture-tools.html\"\u003einternal architecture tools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eImplementation and research of small prototypes\u003c/li\u003e\n\u003cli\u003eData analysis\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"education\"\u003eEDUCATION\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eM.Sc, INFORMATION SYSTEMS, 2013, \u003ca href=\"http://publica.fraunhofer.de/documents/N-330459.html\"\u003eLive-visualization of source code artifacts for a software engineering-demonstrator\n\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eB.Sc, COMPUTER SCIENCE, 2009, \u003ca href=\"http://publica.fraunhofer.de/documents/N-192404.html\"\u003eDesign and automation of a release engineering process\n\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/cv/","title":"CV"},{"content":"\u003cp\u003eHi! After living in Köln for almost ten years, I recently moved further up the Rhine to Düsseldorf. I\u0026rsquo;m currently working as a Senior Cloud Engineer, but I\u0026rsquo;m kind of a \u003cem\u003ejack of all trades\u003c/em\u003e, having worked in several similar positions and generally being interested in various technical topics.\u003c/p\u003e\n\u003cp\u003eI also like to invest a good portion of my spare time on technology: coding, reading technical books, running (semi-)professional home labs, trying to keep up with new technologies or paradigms – or short: practicing continuous improvement. Also, I\u0026rsquo;m a Linux and open source advocate. After figuring out I\u0026rsquo;ve been using Linux \u003ca href=\"https://en.opensuse.org/Archive:SuSE_Linux_7.0\"\u003efor two decades\u003c/a\u003e, I realized I\u0026rsquo;m kind of old.\u003c/p\u003e\n\u003cp\u003eTopics unrelated to tech that I care about:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.last.fm/user/bob_terwilliger\"\u003eMusic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://yummy.soerenschneider.com\"\u003eCooking\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTaking photos\u003c/li\u003e\n\u003cli\u003eLiterature\u003c/li\u003e\n\u003cli\u003eHiking, running, playing football and recently yoga as well\u003c/li\u003e\n\u003cli\u003eTea! I accumulated quite a collection of Chinese, Japanese and Taiwanese teas\u003c/li\u003e\n\u003c/ul\u003e\n","description":null,"image":null,"permalink":"https://hugo-profile.netlify.app/about/","title":"ABOUT ME"}]